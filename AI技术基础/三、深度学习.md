## 深度学习概念总结

### 1) 神经网络原理
- **神经网络**：由多层“线性变换 + 非线性激活”组成的函数，用来把输入映射到输出（分类/回归/生成等）。
- **参数（权重/偏置）**：模型需要学习的数值，决定网络如何计算。
- **前向传播（Forward）**：输入数据经过各层计算得到输出预测。
- **损失函数（Loss）**：衡量预测与目标差多少；训练时希望把它变小（分类常用交叉熵，回归常用 MSE/MAE）。
- **反向传播（Backprop）**：从损失出发计算每个参数的梯度（参数对损失的影响），用于更新参数。
- **优化器（Optimizer）**：根据梯度更新参数的规则（如 SGD、Adam）。

---

### 2) 常见网络架构

#### CNN（卷积神经网络）
- **是什么**：用卷积在局部区域提取特征，参数共享，特别擅长处理**图像/网格数据**。
- **直观理解**：像一组小滤镜在图上滑动，捕捉边缘、纹理、形状等局部模式。

#### RNN（循环神经网络）
- **是什么**：用于**序列数据**（文本、时间序列），通过“隐藏状态”把前面信息传到后面。
- **特点**：能建模顺序关系，但长序列可能出现梯度消失/爆炸；常见改进是 **LSTM/GRU**。

#### Transformer
- **是什么**：用**自注意力（Self-Attention）**建模序列中“哪些位置对当前更重要”，并且可并行训练。
- **优势**：更擅长长距离依赖，训练效率高，是 NLP 乃至多模态的主流架构。

#### BERT / GPT（Transformer 的两条典型路线）
- **BERT（Encoder 为主）**：更偏“理解类任务”；常用预训练方式是**掩码预测（Masked LM）**，适合分类、抽取、匹配等。
- **GPT（Decoder 为主）**：更偏“生成类任务”；用**自回归（预测下一个 token）**训练，适合续写、对话、生成等。

---

### 3) 训练流程
- **数据准备**：清洗、划分训练/验证/测试；文本分词、图像归一化等。
- **定义模型与损失**：选网络结构，确定对应任务的损失函数。
- **训练循环**：前向传播 → 计算损失 → 反向传播求梯度 → 优化器更新参数。
- **验证与调参**：在验证集观察指标，调整学习率、batch size、正则化等超参数。
- **防过拟合手段**：正则化、Dropout、数据增强、早停（Early Stopping）等。
- **最终评估**：用测试集做最终效果评估。

---

### 4) 模型部署
- **推理服务**：把模型部署到服务器/端侧，提供 API 或本地调用进行预测/生成。
- **性能优化**：让模型更快/更省（量化 FP16/INT8、裁剪、蒸馏、编译加速如 TensorRT/ONNX Runtime）。
- **工程化关注点**：
  - **延迟**（单次响应时间）、**吞吐**（每秒处理请求数）
  - **成本**（算力/资源）
  - **稳定性与监控**（错误率、超时、数据分布漂移）
  - **版本管理**（灰度发布、回滚）


  ###  CNN / RNN / Transformer 对比（功能与应用场景）

| 维度 | CNN | RNN（含 LSTM/GRU） | Transformer |
|---|---|---|---|
| **核心机制** | 卷积：局部感受野、参数共享 | 递归：隐藏状态逐步传递 | 注意力：全局交互建模 |
| **擅长建模** | **局部空间模式**（边缘、纹理） | **顺序/时间依赖**（短-中序列） | **长距离依赖/全局关系** |
| **并行能力** | 高（卷积可并行） | 低（时间步依赖） | 高（注意力可并行） |
| **长序列能力** | 不是主要强项（需堆叠/扩感受野） | 容易梯度问题（LSTM/GRU缓解） | 强（但标准注意力计算随长度增长较快） |
| **典型输入形态** | 图像/网格（2D/3D），也可 1D 序列 | 序列、时间序列 | 序列、token 化数据（文本/图像patch/视频patch） |
| **常见应用** | 图像分类/检测/分割、OCR、视频特征 | 语音/时间序列、早期 NLP、流式处理 | NLP（BERT/GPT）、多模态、视觉Transformer、生成模型骨干 |
| **常见取舍** | 高效、稳、对数据需求相对低 | 流式友好但训练慢、长依赖弱 | 上限高、全局强但更吃算力/数据（视实现而定） |

**Summary**
- **CNN**：看“局部”很强（图像局部特征）。  
- **RNN**：按“顺序”一步步读（序列/流式）。  
- **Transformer**：每个位置都能“看全局”（长依赖/大模型主流）。  
